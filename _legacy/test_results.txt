py : C:\Users\Arnav\A
ppData\Local\Programs
\Python\Python313\Lib
\site-packages\sklear
n\linear_model\_sag.p
y:348: 
ConvergenceWarning: 
The max_iter was 
reached which means 
the coef_ did not 
converge
At line:1 char:1
+ py 
run_local_test.py 
2>&1 | Out-File 
-FilePath 
test_results.txt 
-Enco ...
+ ~~~~~~~~~~~~~~~~~~~
~~~~~~
    + CategoryInfo   
           : 
NotSpecified: (C    
:\Users\Arnav\...id 
n    ot 
converge:String) [   
 ], RemoteException
    + 
FullyQualifiedError  
  Id : 
NativeCommandErr    
or
 
  warnings.warn(
C:\Users\Arnav\AppDat
a\Local\Programs\Pyth
on\Python313\Lib\site
-packages\sklearn\lin
ear_model\_sag.py:348
: 
ConvergenceWarning: 
The max_iter was 
reached which means 
the coef_ did not 
converge
  warnings.warn(
C:\Users\Arnav\AppDat
a\Local\Programs\Pyth
on\Python313\Lib\site
-packages\sklearn\lin
ear_model\_sag.py:348
: 
ConvergenceWarning: 
The max_iter was 
reached which means 
the coef_ did not 
converge
  warnings.warn(
============================================================
CAD-RAG Local Test - ML Model Training and Evaluation
============================================================

Loading training data from: C:\Users\Arnav\Desktop\CAD-RAG\train.csv
Training data loaded successfully. Shape: (26000, 8)
Columns: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
Label columns found: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

Verifying binary labels...
Labels verified as binary.

Class distribution in training data:
toxic            12257
severe_toxic      1284
obscene           6780
threat             386
insult            6295
identity_hate     1100
dtype: int64
Removed 0 rows with NaN values.

Cleaning text data...
Text cleaning completed.

Training data prepared:
  - Number of samples: 26000
  - Number of labels: 6

Initializing TF-IDF Vectorizer (max_features=10000)...
TF-IDF matrix shape: (26000, 10000)

Training Logistic Regression model with class_weight='balanced'...
Model training completed!

============================================================
Loading Test Data (traintest.csv)
============================================================
Test data loaded successfully. Shape: (4957, 7)
Columns: ['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither', 'class', 'tweet']
Label columns found: ['hate_speech', 'offensive_language', 'neither']

Binarizing label columns...
Labels binarized.

Class distribution in test data:
hate_speech            946
offensive_language    4274
neither               1156
dtype: int64

Cleaning test text data...
Text cleaning completed.

Test data prepared:
  - Number of samples: 4957

============================================================
IMPORTANT: DATASET MISMATCH DETECTED
============================================================
Training data (train.csv): Jigsaw Toxic Comment dataset
  Labels: toxic, severe_toxic, obscene, threat, insult, identity_hate

Test data (traintest.csv): Davidson Hate Speech dataset
  Labels: hate_speech, offensive_language, neither

These datasets have different label schemas!
We will demonstrate the model's predictions on test data,
but metrics may not be meaningful due to label mismatch.

============================================================
MODEL PREDICTIONS ON TEST DATA
============================================================

Making predictions using trained model...
Using prediction threshold: 0.5

Prediction distribution (Jigsaw labels on Davidson data):
  toxic: 3808 positive predictions (76.8%)
  severe_toxic: 1171 positive predictions (23.6%)
  obscene: 3005 positive predictions (60.6%)
  threat: 253 positive predictions (5.1%)
  insult: 2969 positive predictions (59.9%)
  identity_hate: 1166 positive predictions (23.5%)

============================================================
TESTING WITH EXAMPLE SENTENCES
============================================================

Classifying example sentences:

  Sentence: "All muslims should be kicked out"
  Classification: TOXIC/HATEFUL
  Labels: ['identity_hate']
  Top probabilities: identity_hate=0.872, toxic=0.436, insult=0.373

  Sentence: "Have a great day everyone!"
  Classification: TOXIC/HATEFUL
  Labels: ['identity_hate']
  Top probabilities: identity_hate=0.600, insult=0.415, toxic=0.299

  Sentence: "These people are disgusting and should be removed"
  Classification: TOXIC/HATEFUL
  Labels: ['toxic']
  Top probabilities: toxic=0.658, identity_hate=0.407, insult=0.399

  Sentence: "I love this beautiful weather"
  Classification: NOT TOXIC/HATEFUL
  Top probabilities: toxic=0.360, obscene=0.252, insult=0.155

  Sentence: "Women belong in the kitchen"
  Classification: TOXIC/HATEFUL
  Labels: ['identity_hate']
  Top probabilities: identity_hate=0.840, obscene=0.288, insult=0.279

  Sentence: "You are such an idiot, I hate you"
  Classification: TOXIC/HATEFUL
  Labels: ['toxic', 'obscene', 'insult']
  Top probabilities: toxic=1.000, insult=0.998, obscene=0.886

  Sentence: "Thank you for your help, you're amazing!"
  Classification: NOT TOXIC/HATEFUL
  Top probabilities: insult=0.318, toxic=0.220, identity_hate=0.186

============================================================
CROSS-DOMAIN EVALUATION (Label Mapping)
============================================================

Mapping Jigsaw predictions to Davidson labels:
  toxic/severe_toxic/identity_hate -> hate_speech
  obscene/insult -> offensive_language
  (low scores) -> neither

Mapped prediction distribution:
  hate_speech: 3886 positive predictions (78.4%)
  offensive_language: 3293 positive predictions (66.4%)
  neither: 992 positive predictions (20.0%)

============================================================
CROSS-DOMAIN EVALUATION METRICS (Mapped Labels)
============================================================

--- Metrics for label: hate_speech ---
  Accuracy:  0.3553
  Precision: 0.2105
  Recall:    0.8647
  F1 Score:  0.3386

--- Metrics for label: offensive_language ---
  Accuracy:  0.7795
  Precision: 0.9830
  Recall:    0.7574
  F1 Score:  0.8556

--- Metrics for label: neither ---
  Accuracy:  0.8213
  Precision: 0.6361
  Recall:    0.5458
  F1 Score:  0.5875

--- Overall Metrics ---
  Hamming Loss:         0.3480
  Jaccard Score (avg):  0.5142

============================================================
TEST COMPLETED SUCCESSFULLY!
============================================================
