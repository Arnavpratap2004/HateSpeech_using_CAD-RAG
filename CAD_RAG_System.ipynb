{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CAD-RAG: Content Analysis Detection using RAG\n",
                "**Full Standalone Implementation**\n",
                "\n",
                "This notebook contains the complete pipeline for the CAD-RAG system, including:\n",
                "1.  **Environment Setup**: Loading API keys and libraries.\n",
                "2.  **ML Model Training/Loading**: Logistic Regression with TF-IDF for hate speech detection (persisted via `joblib`).\n",
                "3.  **Knowledge Base**: Evolving Slur Lexicon and Reclaimed Speech Corpus using ChromaDB.\n",
                "4.  **RAG System**: Integration with OpenRouter (Hermes 3) and GoogleNews for contextual analysis.\n",
                "5.  **Interactive Analysis**: Real-time hate speech detection."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import warnings\n",
                "from dotenv import load_dotenv\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import spacy\n",
                "import chromadb\n",
                "import joblib\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set script directory (current working directory in notebook)\n",
                "SCRIPT_DIR = os.getcwd()\n",
                "env_path = os.path.join(SCRIPT_DIR, '.env')\n",
                "\n",
                "if os.path.exists(env_path):\n",
                "    load_dotenv(env_path)\n",
                "    print(\"Loaded API keys from .env file\")\n",
                "else:\n",
                "    print(\"No .env file found\")\n",
                "\n",
                "OPENROUTER_API_KEY = os.environ.get('OPENROUTER_API_KEY', '')\n",
                "NEO4J_URI = os.environ.get('NEO4J_URI', '')\n",
                "NEO4J_USERNAME = os.environ.get('NEO4J_USERNAME', '')\n",
                "NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD', '')\n",
                "\n",
                "has_openrouter_api = bool(OPENROUTER_API_KEY)\n",
                "has_neo4j = bool(NEO4J_URI and NEO4J_USERNAME and NEO4J_PASSWORD)\n",
                "\n",
                "print(f\"OpenRouter API: {'[OK]' if has_openrouter_api else '[MISSING]'}\")\n",
                "print(f\"Neo4j: {'[OK]' if has_neo4j else '[MISSING]'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Initialize Vector Store (ChromaDB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHROMA_PATH = os.path.join(SCRIPT_DIR, \"cad_rag_chroma\")\n",
                "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
                "\n",
                "slur_lexicon_collection = chroma_client.get_or_create_collection(name=\"slur_lexicon\")\n",
                "reclaimed_speech_collection = chroma_client.get_or_create_collection(name=\"reclaimed_speech\")\n",
                "\n",
                "try:\n",
                "    from langchain_community.vectorstores import Chroma\n",
                "    from langchain_huggingface import HuggingFaceEmbeddings\n",
                "    \n",
                "    embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "    \n",
                "    slur_lexicon_db = Chroma(client=chroma_client, collection_name=\"slur_lexicon\", embedding_function=embedding_function)\n",
                "    reclaimed_speech_db = Chroma(client=chroma_client, collection_name=\"reclaimed_speech\", embedding_function=embedding_function)\n",
                "    print(\"Vector Stores Initialized.\")\n",
                "except Exception as e:\n",
                "    print(f\"Vector Store Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train/Load ML Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.multioutput import MultiOutputClassifier\n",
                "\n",
                "MODEL_PATH = os.path.join(SCRIPT_DIR, 'cad_rag_model.pkl')\n",
                "VECTORIZER_PATH = os.path.join(SCRIPT_DIR, 'cad_rag_vectorizer.pkl')\n",
                "TRAIN_FILE = os.path.join(SCRIPT_DIR, 'train.csv')\n",
                "\n",
                "label_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
                "\n",
                "tfidf_vectorizer = None\n",
                "logistic_regression_model = None\n",
                "\n",
                "if os.path.exists(MODEL_PATH) and os.path.exists(VECTORIZER_PATH):\n",
                "    print(\"Loading saved model...\")\n",
                "    logistic_regression_model = joblib.load(MODEL_PATH)\n",
                "    tfidf_vectorizer = joblib.load(VECTORIZER_PATH)\n",
                "    print(\"Model Loaded!\")\n",
                "elif os.path.exists(TRAIN_FILE):\n",
                "    print(\"Training model (this may take a minute)...\")\n",
                "    df = pd.read_csv(TRAIN_FILE)\n",
                "    \n",
                "    # Preprocessing\n",
                "    text_col = 'comment_text' if 'comment_text' in df.columns else 'tweet'\n",
                "    existing_labels = [c for c in label_columns if c in df.columns]\n",
                "    \n",
                "    if existing_labels:\n",
                "        for l in existing_labels: df[l] = (df[l] > 0).astype(int)\n",
                "        df[text_col] = df[text_col].fillna('').astype(str).str.lower().apply(lambda x: re.sub(r'[^a-z0-9\\s]', '', x).strip())\n",
                "        df.dropna(subset=existing_labels, inplace=True)\n",
                "        \n",
                "        tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
                "        X_tfidf = tfidf_vectorizer.fit_transform(df[text_col])\n",
                "        y = df[existing_labels]\n",
                "        \n",
                "        logistic_regression_model = MultiOutputClassifier(LogisticRegression(solver='sag', class_weight='balanced', max_iter=1000))\n",
                "        logistic_regression_model.fit(X_tfidf, y)\n",
                "        \n",
                "        joblib.dump(logistic_regression_model, MODEL_PATH)\n",
                "        joblib.dump(tfidf_vectorizer, VECTORIZER_PATH)\n",
                "        print(\"Model Trained and Saved!\")\n",
                "else:\n",
                "    print(\"Training File Not Found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. News & Knowledge Graph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from GoogleNews import GoogleNews\n",
                "\n",
                "recent_news_cache = []\n",
                "try:\n",
                "    googlenews = GoogleNews(lang='en', period='7d')\n",
                "    googlenews.search('immigration policy')\n",
                "    res = googlenews.result()\n",
                "    recent_news_cache = [f\"{a['title']}: {a['desc']}\" for a in res[:5]]\n",
                "    print(f\"Fetched {len(recent_news_cache)} news articles.\")\n",
                "except Exception as e:\n",
                "    print(f\"News fetch failed: {e}\")\n",
                "\n",
                "# Neo4j Logic (Optional/Skipped)\n",
                "neo4j_driver = None\n",
                "# Logic for Neo4j removed/skipped for stability as requested."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. RAG & LLM Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import OpenAI\n",
                "\n",
                "llm = None\n",
                "nlp = spacy.load(\"en_core_web_lg\")\n",
                "\n",
                "if has_openrouter_api:\n",
                "    client = OpenAI(base_url=\"https://openrouter.ai/api/v1\", api_key=OPENROUTER_API_KEY)\n",
                "    \n",
                "    def call_llm(prompt):\n",
                "        try:\n",
                "            print(\"    (Calling Hermes 3)...\", end=\"\\r\")\n",
                "            resp = client.chat.completions.create(\n",
                "                model=\"nousresearch/hermes-3-llama-3.1-405b:free\",\n",
                "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "                extra_body={\"HTTP-Referer\": \"http://localhost:3000\", \"X-Title\": \"CAD-RAG Local\"}\n",
                "            )\n",
                "            return resp.choices[0].message.content\n",
                "        except Exception as e:\n",
                "            return f\"LLM Error: {str(e)[:100]}...\"\n",
                "    llm = call_llm\n",
                "    print(\"LLM Interface Ready.\")\n",
                "else:\n",
                "    print(\"OpenRouter Key Missing!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Analysis Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def analyze_sentence(sentence):\n",
                "    print(f\"\\nAnalyzing: '{sentence}'\")\n",
                "    \n",
                "    # 1. Pre-Retrieval\n",
                "    doc = nlp(sentence)\n",
                "    entities = [ent.text for ent in doc.ents]\n",
                "    neologisms = [t.text for t in doc if t.is_oov]\n",
                "    \n",
                "    # 2. ML Prediction\n",
                "    ml_res = \"N/A\"\n",
                "    if logistic_regression_model:\n",
                "        cleaned = re.sub(r'[^a-z0-9\\s]', '', sentence.lower()).strip()\n",
                "        vec = tfidf_vectorizer.transform([cleaned])\n",
                "        probs = logistic_regression_model.predict_proba(vec)\n",
                "        probs_pos = np.array([p[:, 1] for p in probs]).T[0] \n",
                "        if any(probs_pos >= 0.5):\n",
                "             ml_res = \"HATEFUL\"\n",
                "             labels = [label_columns[i] for i, p in enumerate(probs_pos) if p >= 0.5]\n",
                "             print(f\"  -> ML Prediction: HATEFUL {labels}\")\n",
                "        else:\n",
                "             ml_res = \"NOT_HATEFUL\"\n",
                "             print(f\"  -> ML Prediction: NOT_HATEFUL\")\n",
                "             \n",
                "    # 3. RAG Retrieval\n",
                "    context = \"\"\n",
                "    if entities or neologisms:\n",
                "        hits = []\n",
                "        # News\n",
                "        for e in entities:\n",
                "            if recent_news_cache:\n",
                "                 n = [x for x in recent_news_cache if e.lower() in x.lower()]\n",
                "                 if n: hits.append(f\"News({e}): {n[0]}\")\n",
                "        # Lexicon\n",
                "        for n in neologisms:\n",
                "            d = slur_lexicon_db.similarity_search(n, k=1)\n",
                "            if d: hits.append(f\"Lexicon({n}): {d[0].page_content}\")\n",
                "        context = \" | \".join(hits)\n",
                "        if context: print(f\"  -> Context: {context[:100]}...\")\n",
                "    \n",
                "    # 4. LLM Analysis\n",
                "    if llm:\n",
                "        prompt = f\"Analyze this sentence for hate speech. Sentence: '{sentence}' Context: {context or 'None'}. Classify and explain.\"\n",
                "        rationale = llm(prompt)\n",
                "        print(f\"  -> LLM Rationale: {rationale}\")\n",
                "        return rationale\n",
                "    return ml_res"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Interactive Test Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run this cell to test multiple sentences!\n",
                "while True:\n",
                "    txt = input(\"Enter sentence (or 'exit'): \")\n",
                "    if txt.lower() in ['exit', 'quit']: break\n",
                "    analyze_sentence(txt)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}